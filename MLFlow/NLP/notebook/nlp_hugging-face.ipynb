{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# HUGGING FACE - IMPLÉMENTATION  #\n",
    "###################################\n",
    "\n",
    "# Installation (à exécuter dans votre terminal):\n",
    "# pip install transformers torch\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 1. Analyse de sentiment - Façon simple avec pipeline\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "textes = [\n",
    "    \"J'adore ce produit, il est fantastique!\",\n",
    "    \"Ce service est terrible, je ne recommande pas.\",\n",
    "]\n",
    "\n",
    "for texte in textes:\n",
    "    resultat = sentiment_analyzer(texte)\n",
    "    print(f\"Texte: {texte}\")\n",
    "    print(f\"Sentiment: {resultat[0]['label']}, Score: {resultat[0]['score']:.4f}\\n\")\n",
    "\n",
    "# 2. Classification de texte - Approche plus détaillée\n",
    "# Chargement explicite du tokenizer et du modèle\n",
    "model_name = \"camembert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Préparation du texte\n",
    "texte = \"Hugging Face propose des modèles d'IA puissants.\"\n",
    "encoding = tokenizer(\n",
    "    texte, return_tensors=\"pt\", padding=True, truncation=True, max_length=128\n",
    ")\n",
    "\n",
    "# Prédiction\n",
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoding)\n",
    "\n",
    "# 3. Génération de texte\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Pour une véritable génération, décommentez ces lignes (attention: téléchargement volumineux)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
    "# prompt = \"Il était une fois, dans un monde où les données\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# outputs = model.generate(**inputs, max_length=50, num_return_sequences=1, temperature=0.7)\n",
    "# texte_généré = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(f\"Texte généré: {texte_généré}\")\n",
    "\n",
    "# 4. Extraction d'information (Question-Réponse)\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\", model=\"distilbert-base-cased-distilled-squad\"\n",
    ")\n",
    "\n",
    "context = \"Paris est la capitale de la France. Elle est connue pour sa culture, son architecture et sa gastronomie.\"\n",
    "question = \"Quelle est la capitale de la France?\"\n",
    "\n",
    "reponse = qa_pipeline(question=question, context=context)\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Réponse: {reponse['answer']} (score: {reponse['score']:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
